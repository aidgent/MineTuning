{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a mine-tuning take on the \"ChatGPT\"-style chatbot UX, with quantitative and qualitative feedback at each interaction: 5-star rating and feedback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dspy-ai\n",
    "%pip install ipywidgets\n",
    "%pip install IPython\n",
    "%pip install requests\n",
    "%pip install markdownify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import os\n",
    "import dotenv\n",
    "import pydantic\n",
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "dotenv.load_dotenv() #load via .env in this folder (.env is in .gitignore)\n",
    "#os.environ['OPENAI_API_KEY'] = 'sk-YOUR_OPENAI_API_KEY' #or set directly here, just remember not to commit to GitHub\n",
    "assert 'OPENAI_API_KEY' in os.environ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this custom class is needed to append the chat history, which is not available to DSPy's OpenAI class (dsp>modules>GPT3.py) by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.modules.gpt3 import (\n",
    "    _cached_gpt3_turbo_request_v2_wrapped,\n",
    "    v1_cached_gpt3_request_v2,\n",
    "    v1_cached_gpt3_request_v2_wrapped,\n",
    "    v1_cached_gpt3_turbo_request_v2,\n",
    "    v1_cached_gpt3_turbo_request_v2_wrapped,\n",
    "    chat_request,\n",
    "    completions_request,\n",
    "    GPT3\n",
    ")\n",
    "\n",
    "class ChatGPT(dspy.OpenAI):\n",
    "    def __init__(self, model='gpt-3.5-turbo', show_feedback=True, api_key=None, **kwargs):\n",
    "        super().__init__(model=model, api_key=api_key, **kwargs)\n",
    "        self.system_prompt = None\n",
    "        self.model = model\n",
    "        self.show_feedback = show_feedback\n",
    "\n",
    "    def basic_request(self, prompt: str, rating=None, feedback=None, **kwargs):\n",
    "        raw_kwargs = kwargs\n",
    "\n",
    "        # Prepare the messages with history\n",
    "        messages = [{\"role\": \"user\", \"content\": entry[\"prompt\"]} for entry in self.history]\n",
    "        messages += [{\"role\": \"assistant\", \"content\": self._get_choice_text(entry[\"response\"][\"choices\"][0])} for entry in self.history]\n",
    "        \n",
    "        #if we want the bot to see the rating and feedback, append them to the prompt\n",
    "        if self.show_feedback:\n",
    "             # Append the current prompt\n",
    "            if rating or feedback:\n",
    "                prompt_with_feedback = prompt + \"\\n---\\nHere is the users rating (1-5) and feedback of your last response:\"\n",
    "                if rating is not None:\n",
    "                    prompt_with_feedback += f\"\\n\\nRating (1-5): {rating}\"\n",
    "                if feedback is not None:\n",
    "                    prompt_with_feedback += f\"\\n\\nFeedback: {feedback}\\n---\\n\"\n",
    "                messages.append({\"role\": \"user\", \"content\": prompt_with_feedback})\n",
    "            else:\n",
    "                messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "\n",
    "\n",
    "        if self.system_prompt:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": self.system_prompt})\n",
    "        \n",
    "        # Remove feedback and rating from kwargs\n",
    "        kwargs.pop('feedback', None)\n",
    "        kwargs.pop('rating', None)\n",
    "\n",
    "        # add self and model to kwargs\n",
    "        kwargs[\"messages\"] = messages\n",
    "        kwargs[\"model\"] = self.model\n",
    "        kwargs = {\"stringify_request\": json.dumps(kwargs)}\n",
    "        response = chat_request(**kwargs)\n",
    "\n",
    "        history = {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"kwargs\": kwargs,\n",
    "            \"raw_kwargs\": raw_kwargs,\n",
    "        }\n",
    "        self.history.append(history)\n",
    "\n",
    "        return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this simple example shows what is going on under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'message one:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "12 + 12 equals 24."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'message two:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Your last message was: \"tell me what is 12+12?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'message three:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "In a peaceful village, 12 kind hearts met another 12 kind hearts, creating a community of 24 caring souls. Together, they built a beautiful garden where laughter and love blossomed. The village thrived, showing that when people unite, magic happens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'now show the history, in reverse order to make it easier to see the conversation flow'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'now tell me a very short story based on the answer of the first message, ideally a 3 sentence story',\n",
       "  'response': {'id': 'chatcmpl-9SP2CTz7FII7T6op9hAbNgwrkZqVK',\n",
       "   'choices': [{'finish_reason': 'stop',\n",
       "     'index': 0,\n",
       "     'logprobs': None,\n",
       "     'message': {'content': 'In a peaceful village, 12 kind hearts met another 12 kind hearts, creating a community of 24 caring souls. Together, they built a beautiful garden where laughter and love blossomed. The village thrived, showing that when people unite, magic happens.',\n",
       "      'role': 'assistant',\n",
       "      'function_call': None,\n",
       "      'tool_calls': None}}],\n",
       "   'created': 1716556444,\n",
       "   'model': 'gpt-4o-2024-05-13',\n",
       "   'object': 'chat.completion',\n",
       "   'system_fingerprint': 'fp_729ea513f7',\n",
       "   'usage': {'completion_tokens': 53,\n",
       "    'prompt_tokens': 83,\n",
       "    'total_tokens': 136}},\n",
       "  'kwargs': {'stringify_request': '{\"messages\": [{\"role\": \"user\", \"content\": \"tell me what is 12+12?\"}, {\"role\": \"user\", \"content\": \"what was my last message?\"}, {\"role\": \"assistant\", \"content\": \"12 + 12 equals 24.\"}, {\"role\": \"assistant\", \"content\": \"Your last message was: \\\\\"tell me what is 12+12?\\\\\"\"}, {\"role\": \"user\", \"content\": \"now tell me a very short story based on the answer of the first message, ideally a 3 sentence story\"}], \"model\": \"gpt-4o\"}'},\n",
       "  'raw_kwargs': {'messages': [{'role': 'user',\n",
       "     'content': 'tell me what is 12+12?'},\n",
       "    {'role': 'user', 'content': 'what was my last message?'},\n",
       "    {'role': 'assistant', 'content': '12 + 12 equals 24.'},\n",
       "    {'role': 'assistant',\n",
       "     'content': 'Your last message was: \"tell me what is 12+12?\"'},\n",
       "    {'role': 'user',\n",
       "     'content': 'now tell me a very short story based on the answer of the first message, ideally a 3 sentence story'}],\n",
       "   'model': 'gpt-4o'}},\n",
       " {'prompt': 'what was my last message?',\n",
       "  'response': {'id': 'chatcmpl-9SHBIXsEoYjq6F3qDgwcQCaNzbfbs',\n",
       "   'choices': [{'finish_reason': 'stop',\n",
       "     'index': 0,\n",
       "     'logprobs': None,\n",
       "     'message': {'content': 'Your last message was: \"tell me what is 12+12?\"',\n",
       "      'role': 'assistant',\n",
       "      'function_call': None,\n",
       "      'tool_calls': None}}],\n",
       "   'created': 1716526256,\n",
       "   'model': 'gpt-4o-2024-05-13',\n",
       "   'object': 'chat.completion',\n",
       "   'system_fingerprint': 'fp_729ea513f7',\n",
       "   'usage': {'completion_tokens': 15,\n",
       "    'prompt_tokens': 38,\n",
       "    'total_tokens': 53}},\n",
       "  'kwargs': {'stringify_request': '{\"messages\": [{\"role\": \"user\", \"content\": \"tell me what is 12+12?\"}, {\"role\": \"assistant\", \"content\": \"12 + 12 equals 24.\"}, {\"role\": \"user\", \"content\": \"what was my last message?\"}], \"model\": \"gpt-4o\"}'},\n",
       "  'raw_kwargs': {'messages': [{'role': 'user',\n",
       "     'content': 'tell me what is 12+12?'},\n",
       "    {'role': 'assistant', 'content': '12 + 12 equals 24.'},\n",
       "    {'role': 'user', 'content': 'what was my last message?'}],\n",
       "   'model': 'gpt-4o'}},\n",
       " {'prompt': 'tell me what is 12+12?',\n",
       "  'response': {'id': 'chatcmpl-9SHBIYd8AVf0uqeNtzcANOTzrnTHw',\n",
       "   'choices': [{'finish_reason': 'stop',\n",
       "     'index': 0,\n",
       "     'logprobs': None,\n",
       "     'message': {'content': '12 + 12 equals 24.',\n",
       "      'role': 'assistant',\n",
       "      'function_call': None,\n",
       "      'tool_calls': None}}],\n",
       "   'created': 1716526256,\n",
       "   'model': 'gpt-4o-2024-05-13',\n",
       "   'object': 'chat.completion',\n",
       "   'system_fingerprint': 'fp_729ea513f7',\n",
       "   'usage': {'completion_tokens': 8, 'prompt_tokens': 16, 'total_tokens': 24}},\n",
       "  'kwargs': {'stringify_request': '{\"messages\": [{\"role\": \"user\", \"content\": \"tell me what is 12+12?\"}], \"model\": \"gpt-4o\"}'},\n",
       "  'raw_kwargs': {'messages': [{'role': 'user',\n",
       "     'content': 'tell me what is 12+12?'}],\n",
       "   'model': 'gpt-4o'}}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = ChatGPT(model='gpt-4o', show_feedback=False, temperature=0.1, max_tokens=4096)\n",
    "message_one = llm(\"tell me what is 12+12?\")\n",
    "display(\"message one:\")\n",
    "display(Markdown(message_one[0]))\n",
    "# display(\"message one history:\")\n",
    "# display(llm.history)\n",
    "message_two = llm(\"what was my last message?\")\n",
    "display(\"message two:\")\n",
    "display(Markdown(message_two[0]))\n",
    "#display(\"message two history:\")\n",
    "#display(llm.history)\n",
    "message_three = llm(\"now tell me a very short story based on the answer of the first message, ideally a 3 sentence story\")\n",
    "display(\"message three:\")\n",
    "display(Markdown(message_three[0]))\n",
    "display(\"now show the history, in reverse order to make it easier to see the conversation flow\")\n",
    "display(llm.history[::-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's enable feedback now to get ready to chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGPT(model='gpt-4o', show_feedback=True, temperature=0.1, max_tokens=4096)\n",
    "llm.system_prompt = \"always format responses in markdown\"\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally clear the history\n",
    "llm.history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this widget is a (clunky) \"ChatGPT\" interface, with feedback at each engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**User:** tell me a story."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** In a small seaside village, there lived an elderly fisherman named Elias. Elias was known for his kindness and generosity, often sharing his modest catch with his neighbors. His most prized possession was a beautifully crafted, but old, fishing boat that had been passed down through generations. He named it \"Starfish.\"\n",
       "\n",
       "One stormy night, a bright flash of lightning struck the ocean, illuminating a strange but beautiful creature entangled in a mass of seaweed. The creature, with iridescent scales and eyes that shimmered like pearls, was unlike anything Elias had ever seen. Without hesitation, Elias sailed out in \"Starfish\" to rescue it. He carefully untangled the creature and brought it aboard, treating its wounds with gentle care.\n",
       "\n",
       "To Elias’s astonishment, the creature revealed itself to be a mermaid named Marina. Grateful for his kindness, Marina offered to grant Elias one wish. Elias, humble and selfless, asked only for the well-being of his village, which had been struggling to thrive. With a wave of her hand, Marina summoned a plentiful bounty from the sea, filling Elias’s nets with fish. She also enchanted \"Starfish,\" ensuring it would always lead him to fruitful waters.\n",
       "\n",
       "From that day on, the village prospered, and Elias became a beloved local legend. The villagers spoke of the magical night when a mermaid changed their fortunes forever, and \"Starfish\" became a symbol of hope and generosity. Elias, content with the knowledge that he had secured a bright future for his people, continued to sail the tranquil seas, always on the lookout for the next soul in need."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User:** a very short story"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Once in a serene meadow, a baby llama named Luna noticed her friends looked sad. She discovered their favorite patch of flowers had wilted. Determined to help, Luna trekked to the highest hill where a magical spring flowed.\n",
       "\n",
       "Under the moonlight, she gently filled her tiny bucket with the spring water and returned to the meadow. Pouring the magical water onto the flower patch, Luna watched as the flowers bloomed brighter than ever.\n",
       "\n",
       "Her friends cheered, their spirits lifted by Luna's bravery and kindness. And so, Luna the baby llama became their hero, proving that even the smallest actions can bring immense joy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User:** another short story"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** In a distant future, on a colossal space station orbiting the icy planet of Zephyria, lived a young engineer named Kai. Kai had always been fascinated by the vastness of space and the intricate machinery that kept their space station operational. His prized possession was a small, ancient robotic companion named Vector, which he had refurbished from a junkyard.\n",
       "\n",
       "One day, the space station's power core began to fail, threatening the lives of everyone aboard. As panic spread, Kai and Vector sprang into action. With limited time, they navigated the labyrinthine corridors to reach the core's control room. Using his engineering skills, Kai realized that to save the station, he needed a rare crystal only found on Zephyria's surface.\n",
       "\n",
       "Risking everything, Kai and Vector boarded a small shuttle and descended through the planet's harsh atmosphere. Battling freezing winds and treacherous terrain, they finally located the crystal in a cavern beneath the ice. Vector's sensors guided Kai through the dark tunnels, and together, they retrieved the precious crystal.\n",
       "\n",
       "Racing against time, they returned to the space station and installed the crystal in the power core just moments before total shutdown. The lights flickered back to life, and the station's inhabitants erupted in cheers. Kai's bravery and ingenuity, along with Vector's unwavering assistance, had saved them all.\n",
       "\n",
       "From that day on, Kai was celebrated as a hero, proving that even in the vast loneliness of space, determination and teamwork could overcome any challenge. His story became a beacon of hope, inspiring future generations to look to the stars with courage and curiosity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User:** redo this and remember my preferences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** In a distant future, on a colossal space station orbiting the icy planet of Zephyria, lived a baby llama named Luna. Luna had always been fascinated by the vastness of space and the intricate machinery that kept their space station operational. Her prized possession was a small, ancient robotic companion named Vector, which she had refurbished from a junkyard.\n",
       "\n",
       "One day, the space station's power core began to fail, threatening the lives of everyone aboard. As panic spread, Luna and Vector sprang into action. Using her surprising engineering skills, Luna navigated the labyrinthine corridors to reach the core's control room. She quickly realized that to save the station, she needed a rare crystal only found on Zephyria's surface.\n",
       "\n",
       "Risking everything, Luna and Vector boarded a small shuttle and descended through the planet's harsh atmosphere. Battling freezing winds and treacherous terrain, they finally located the crystal in a cavern beneath the ice. Vector's sensors guided Luna through the dark tunnels, and together, they retrieved the precious crystal.\n",
       "\n",
       "Racing against time, Luna returned to the space station and installed the crystal in the power core just moments before a total shutdown. The lights flickered back to life, and the station's inhabitants erupted in cheers. Luna's bravery and ingenuity, along with Vector's unwavering assistance, had saved them all.\n",
       "\n",
       "From that day on, Luna the baby llama was celebrated as a hero. Her story proved that even in the vast loneliness of space, determination and teamwork could overcome any challenge. Luna's tale became a beacon of hope, inspiring future generations to look to the stars with courage and curiosity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User:** tell me my preferences"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** Based on your high rating and positive feedback, it seems you prefer:\n",
       "1. **Short yet comprehensive stories** – Narrative that is rich but not overly long.\n",
       "2. **Unique settings** – Stories set in interesting and imaginative worlds.\n",
       "3. **Adventurous plotlines** – Stories featuring determination, courage, and overcoming challenges.\n",
       "4. **Character-driven narratives** – Featuring memorable and engaging characters like baby llamas or intriguing sidekicks like ancient robots. \n",
       "\n",
       "I will keep these preferences in mind for future stories. Would you like another one?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4270ae36ed44a5ab9cdaeac4a4216c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center><p>Rating</p></center>'), HBox(children=(Button(description='1', style=Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1d3917215445b69bca58433fff00e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Feedback:', layout=Layout(height='100px', width='100%'), placeholder='Type you…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63691c7f05042c7b805c240e90ef8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Query:', layout=Layout(height='100px', width='100%'), placeholder='Type your m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c113ae618a40b08fd535cfee18973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Submit', style=ButtonStyle()),), layout=Layout(justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown, HTML\n",
    "\n",
    "# Initialize storage for chat history, ratings, and feedback\n",
    "chat_history = []\n",
    "history_with_feedback = []\n",
    "\n",
    "# Variable to store the selected rating\n",
    "selected_rating = None\n",
    "\n",
    "# Create buttons for ratings 1 to 5\n",
    "buttons = [widgets.Button(description=str(i)) for i in range(1, 6)]\n",
    "\n",
    "# Arrange the buttons horizontally and center them\n",
    "button_box = widgets.VBox([\n",
    "    widgets.HTML(\"<center><p>Rating</p></center>\"),\n",
    "    widgets.HBox(buttons, layout=widgets.Layout(justify_content='center'))\n",
    "])\n",
    "\n",
    "# Create a Textarea widget for feedback\n",
    "feedback_area = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your feedback here',\n",
    "    description='Feedback:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a Textarea widget for message\n",
    "message_area = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your message here',\n",
    "    description='Query:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a submit button\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "# Center the submit button\n",
    "submit_button_box = widgets.HBox([submit_button], layout=widgets.Layout(justify_content='center'))\n",
    "\n",
    "# Function to display the chat history and input widgets\n",
    "def display_chat():\n",
    "    clear_output()\n",
    "    \n",
    "    # Display the chat history\n",
    "    for entry in chat_history:\n",
    "        display(Markdown(f\"**User:** {entry['message']}\"))\n",
    "        display(Markdown(f\"**Response:** {entry['response']}\"))\n",
    "    \n",
    "        \n",
    "    # Display the rating buttons and feedback area only if there is message history\n",
    "    if chat_history:\n",
    "        display(button_box)\n",
    "        display(feedback_area)\n",
    "\n",
    "    # Display the message area\n",
    "    display(message_area)\n",
    "    \n",
    "    # Display the submit button\n",
    "    display(submit_button_box)\n",
    "\n",
    "# Function to handle submit button click\n",
    "def on_submit_click(b):\n",
    "    global selected_rating\n",
    "    \n",
    "    # Get the message and feedback text\n",
    "    message_text = message_area.value\n",
    "    feedback_text = feedback_area.value\n",
    "    \n",
    "    # Display loading message\n",
    "    loading_message = widgets.HTML(\"<center><p>Loading...</p></center>\")\n",
    "    display(loading_message)\n",
    "    \n",
    "    # Simulate getting a response from the model\n",
    "    response = llm(message_text, rating=selected_rating, feedback=feedback_text)\n",
    "    response_text = response[0]\n",
    "    \n",
    "    # Clear loading message\n",
    "    clear_output()\n",
    "    \n",
    "    # Save the chat history\n",
    "    chat_history.append({\n",
    "        'message': message_text,\n",
    "        'response': response_text\n",
    "    })\n",
    "    \n",
    "    # Save the rating and feedback to the last entry in history_with_feedback\n",
    "    if history_with_feedback:\n",
    "        history_with_feedback[-1]['rating'] = selected_rating\n",
    "        history_with_feedback[-1]['feedback'] = feedback_text\n",
    "    history_with_feedback.append({\n",
    "        'query': message_text,\n",
    "        'response': response_text,\n",
    "    })\n",
    "    \n",
    "    # Reset the input fields and selected rating\n",
    "    message_area.value = ''\n",
    "    feedback_area.value = ''\n",
    "    selected_rating = None\n",
    "    \n",
    "    # Reset button styles\n",
    "    for button in buttons:\n",
    "        button.style.button_color = None\n",
    "    \n",
    "    # Display the updated chat\n",
    "    display_chat()\n",
    "\n",
    "# Function to handle button click\n",
    "def on_button_click(b):\n",
    "    global selected_rating\n",
    "    selected_rating = int(b.description)\n",
    "    \n",
    "    # Reset button styles\n",
    "    for button in buttons:\n",
    "        button.style.button_color = None\n",
    "    \n",
    "    # Highlight the selected button\n",
    "    b.style.button_color = 'lightblue'\n",
    "    #print(f\"Selected rating: {selected_rating}\")\n",
    "\n",
    "# Attach the button click event to each button\n",
    "for button in buttons:\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "# Attach the submit button click event\n",
    "submit_button.on_click(on_submit_click)\n",
    "\n",
    "# Display the initial chat interface\n",
    "display_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we might want to save this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'tell me a story.',\n",
       "  'response': 'In a small seaside village, there lived an elderly fisherman named Elias. Elias was known for his kindness and generosity, often sharing his modest catch with his neighbors. His most prized possession was a beautifully crafted, but old, fishing boat that had been passed down through generations. He named it \"Starfish.\"\\n\\nOne stormy night, a bright flash of lightning struck the ocean, illuminating a strange but beautiful creature entangled in a mass of seaweed. The creature, with iridescent scales and eyes that shimmered like pearls, was unlike anything Elias had ever seen. Without hesitation, Elias sailed out in \"Starfish\" to rescue it. He carefully untangled the creature and brought it aboard, treating its wounds with gentle care.\\n\\nTo Elias’s astonishment, the creature revealed itself to be a mermaid named Marina. Grateful for his kindness, Marina offered to grant Elias one wish. Elias, humble and selfless, asked only for the well-being of his village, which had been struggling to thrive. With a wave of her hand, Marina summoned a plentiful bounty from the sea, filling Elias’s nets with fish. She also enchanted \"Starfish,\" ensuring it would always lead him to fruitful waters.\\n\\nFrom that day on, the village prospered, and Elias became a beloved local legend. The villagers spoke of the magical night when a mermaid changed their fortunes forever, and \"Starfish\" became a symbol of hope and generosity. Elias, content with the knowledge that he had secured a bright future for his people, continued to sail the tranquil seas, always on the lookout for the next soul in need.',\n",
       "  'rating': 2,\n",
       "  'feedback': 'a baby llama must be the hero of every story'},\n",
       " {'query': 'a very short story',\n",
       "  'response': \"Once in a serene meadow, a baby llama named Luna noticed her friends looked sad. She discovered their favorite patch of flowers had wilted. Determined to help, Luna trekked to the highest hill where a magical spring flowed.\\n\\nUnder the moonlight, she gently filled her tiny bucket with the spring water and returned to the meadow. Pouring the magical water onto the flower patch, Luna watched as the flowers bloomed brighter than ever.\\n\\nHer friends cheered, their spirits lifted by Luna's bravery and kindness. And so, Luna the baby llama became their hero, proving that even the smallest actions can bring immense joy.\",\n",
       "  'rating': 4,\n",
       "  'feedback': 'and all stories must be sci-fi'},\n",
       " {'query': 'another short story',\n",
       "  'response': \"In a distant future, on a colossal space station orbiting the icy planet of Zephyria, lived a young engineer named Kai. Kai had always been fascinated by the vastness of space and the intricate machinery that kept their space station operational. His prized possession was a small, ancient robotic companion named Vector, which he had refurbished from a junkyard.\\n\\nOne day, the space station's power core began to fail, threatening the lives of everyone aboard. As panic spread, Kai and Vector sprang into action. With limited time, they navigated the labyrinthine corridors to reach the core's control room. Using his engineering skills, Kai realized that to save the station, he needed a rare crystal only found on Zephyria's surface.\\n\\nRisking everything, Kai and Vector boarded a small shuttle and descended through the planet's harsh atmosphere. Battling freezing winds and treacherous terrain, they finally located the crystal in a cavern beneath the ice. Vector's sensors guided Kai through the dark tunnels, and together, they retrieved the precious crystal.\\n\\nRacing against time, they returned to the space station and installed the crystal in the power core just moments before total shutdown. The lights flickered back to life, and the station's inhabitants erupted in cheers. Kai's bravery and ingenuity, along with Vector's unwavering assistance, had saved them all.\\n\\nFrom that day on, Kai was celebrated as a hero, proving that even in the vast loneliness of space, determination and teamwork could overcome any challenge. His story became a beacon of hope, inspiring future generations to look to the stars with courage and curiosity.\",\n",
       "  'rating': 3,\n",
       "  'feedback': 'you forgot the baby llama must be the hero'},\n",
       " {'query': 'redo this and remember my preferences',\n",
       "  'response': \"In a distant future, on a colossal space station orbiting the icy planet of Zephyria, lived a baby llama named Luna. Luna had always been fascinated by the vastness of space and the intricate machinery that kept their space station operational. Her prized possession was a small, ancient robotic companion named Vector, which she had refurbished from a junkyard.\\n\\nOne day, the space station's power core began to fail, threatening the lives of everyone aboard. As panic spread, Luna and Vector sprang into action. Using her surprising engineering skills, Luna navigated the labyrinthine corridors to reach the core's control room. She quickly realized that to save the station, she needed a rare crystal only found on Zephyria's surface.\\n\\nRisking everything, Luna and Vector boarded a small shuttle and descended through the planet's harsh atmosphere. Battling freezing winds and treacherous terrain, they finally located the crystal in a cavern beneath the ice. Vector's sensors guided Luna through the dark tunnels, and together, they retrieved the precious crystal.\\n\\nRacing against time, Luna returned to the space station and installed the crystal in the power core just moments before a total shutdown. The lights flickered back to life, and the station's inhabitants erupted in cheers. Luna's bravery and ingenuity, along with Vector's unwavering assistance, had saved them all.\\n\\nFrom that day on, Luna the baby llama was celebrated as a hero. Her story proved that even in the vast loneliness of space, determination and teamwork could overcome any challenge. Luna's tale became a beacon of hope, inspiring future generations to look to the stars with courage and curiosity.\",\n",
       "  'rating': 5,\n",
       "  'feedback': 'now this is what i like'},\n",
       " {'query': 'tell me my preferences',\n",
       "  'response': 'Based on your high rating and positive feedback, it seems you prefer:\\n1. **Short yet comprehensive stories** – Narrative that is rich but not overly long.\\n2. **Unique settings** – Stories set in interesting and imaginative worlds.\\n3. **Adventurous plotlines** – Stories featuring determination, courage, and overcoming challenges.\\n4. **Character-driven narratives** – Featuring memorable and engaging characters like baby llamas or intriguing sidekicks like ancient robots. \\n\\nI will keep these preferences in mind for future stories. Would you like another one?'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save = history_with_feedback\n",
    "display(save)\n",
    "import json\n",
    "\n",
    "with open('demo_ratings_feedback.json', 'w') as f:\n",
    "    json.dump(save, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to optimize, we will need a DSPy signature & module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat(dspy.Signature):\n",
    "    \"\"\"You are a helpful assistant that is always improving based on user feedback. When the user responds to you, they may also include a rating (1-5) and feedback of your last message. Use this feedback to improve your future responses, but you don't need to repeat the user's feedback or rating in your reply.\"\"\"\n",
    "\n",
    "    query = dspy.InputField(desc=\"This is the user's current query.\")\n",
    "    rating = dspy.InputField(desc=\"This is the user's rating of your last response.\")\n",
    "    feedback = dspy.InputField(desc=\"This is the user's feedback on your last response.\")\n",
    "    response = dspy.OutputField(desc=\"This is your response to the user's query. Only output your response in Markdown, do not include any additional text.\") #sometimes the AI responds with the output description, so i often leave this blank\n",
    "\n",
    "\n",
    "class ChatModule(dspy.Module):  # let's define a new module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n = 1 #don't do more than 1 right now...\n",
    "        self.signature = Chat\n",
    "        self.predictor = dspy.Predict(self.signature, n=self.n) #this can easily be subbed for some other predictor\n",
    "    \n",
    "    def forward(self, query, rating=None, feedback=None, num_generations = 1):#don't do more than 1 generation right now...\n",
    "        self.predictor = dspy.Predict(self.signature, n=num_generations)\n",
    "        result = self.predictor(query=query, rating=rating, feedback=feedback, n=num_generations)\n",
    "        return dspy.Prediction(response=[str(completion.response) for completion in result.completions])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we need to convert our feedback into a DSPy.example\n",
    "\n",
    "for this demo, if there is no rating we're going to give the neutral rating of 3 just to make sure the compiler works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example({'query': 'tell me a story.', 'response': 'In a small seaside village, there lived an elderly fisherman named Elias. Elias was known for his kindness and generosity, often sharing his modest catch with his neighbors. His most prized possession was a beautifully crafted, but old, fishing boat that had been passed down through generations. He named it \"Starfish.\"\\n\\nOne stormy night, a bright flash of lightning struck the ocean, illuminating a strange but beautiful creature entangled in a mass of seaweed. The creature, with iridescent scales and eyes that shimmered like pearls, was unlike anything Elias had ever seen. Without hesitation, Elias sailed out in \"Starfish\" to rescue it. He carefully untangled the creature and brought it aboard, treating its wounds with gentle care.\\n\\nTo Elias’s astonishment, the creature revealed itself to be a mermaid named Marina. Grateful for his kindness, Marina offered to grant Elias one wish. Elias, humble and selfless, asked only for the well-being of his village, which had been struggling to thrive. With a wave of her hand, Marina summoned a plentiful bounty from the sea, filling Elias’s nets with fish. She also enchanted \"Starfish,\" ensuring it would always lead him to fruitful waters.\\n\\nFrom that day on, the village prospered, and Elias became a beloved local legend. The villagers spoke of the magical night when a mermaid changed their fortunes forever, and \"Starfish\" became a symbol of hope and generosity. Elias, content with the knowledge that he had secured a bright future for his people, continued to sail the tranquil seas, always on the lookout for the next soul in need.', 'rating': '2', 'feedback': 'a baby llama must be the hero of every story'}) (input_keys={'feedback', 'query', 'rating'}),\n",
       " Example({'query': 'a very short story', 'response': \"Once in a serene meadow, a baby llama named Luna noticed her friends looked sad. She discovered their favorite patch of flowers had wilted. Determined to help, Luna trekked to the highest hill where a magical spring flowed.\\n\\nUnder the moonlight, she gently filled her tiny bucket with the spring water and returned to the meadow. Pouring the magical water onto the flower patch, Luna watched as the flowers bloomed brighter than ever.\\n\\nHer friends cheered, their spirits lifted by Luna's bravery and kindness. And so, Luna the baby llama became their hero, proving that even the smallest actions can bring immense joy.\", 'rating': '4', 'feedback': 'and all stories must be sci-fi'}) (input_keys={'feedback', 'query', 'rating'}),\n",
       " Example({'query': 'another short story', 'response': \"In a distant future, on a colossal space station orbiting the icy planet of Zephyria, lived a young engineer named Kai. Kai had always been fascinated by the vastness of space and the intricate machinery that kept their space station operational. His prized possession was a small, ancient robotic companion named Vector, which he had refurbished from a junkyard.\\n\\nOne day, the space station's power core began to fail, threatening the lives of everyone aboard. As panic spread, Kai and Vector sprang into action. With limited time, they navigated the labyrinthine corridors to reach the core's control room. Using his engineering skills, Kai realized that to save the station, he needed a rare crystal only found on Zephyria's surface.\\n\\nRisking everything, Kai and Vector boarded a small shuttle and descended through the planet's harsh atmosphere. Battling freezing winds and treacherous terrain, they finally located the crystal in a cavern beneath the ice. Vector's sensors guided Kai through the dark tunnels, and together, they retrieved the precious crystal.\\n\\nRacing against time, they returned to the space station and installed the crystal in the power core just moments before total shutdown. The lights flickered back to life, and the station's inhabitants erupted in cheers. Kai's bravery and ingenuity, along with Vector's unwavering assistance, had saved them all.\\n\\nFrom that day on, Kai was celebrated as a hero, proving that even in the vast loneliness of space, determination and teamwork could overcome any challenge. His story became a beacon of hope, inspiring future generations to look to the stars with courage and curiosity.\", 'rating': '3', 'feedback': 'you forgot the baby llama must be the hero'}) (input_keys={'feedback', 'query', 'rating'}),\n",
       " Example({'query': 'redo this and remember my preferences', 'response': \"In a distant future, on a colossal space station orbiting the icy planet of Zephyria, lived a baby llama named Luna. Luna had always been fascinated by the vastness of space and the intricate machinery that kept their space station operational. Her prized possession was a small, ancient robotic companion named Vector, which she had refurbished from a junkyard.\\n\\nOne day, the space station's power core began to fail, threatening the lives of everyone aboard. As panic spread, Luna and Vector sprang into action. Using her surprising engineering skills, Luna navigated the labyrinthine corridors to reach the core's control room. She quickly realized that to save the station, she needed a rare crystal only found on Zephyria's surface.\\n\\nRisking everything, Luna and Vector boarded a small shuttle and descended through the planet's harsh atmosphere. Battling freezing winds and treacherous terrain, they finally located the crystal in a cavern beneath the ice. Vector's sensors guided Luna through the dark tunnels, and together, they retrieved the precious crystal.\\n\\nRacing against time, Luna returned to the space station and installed the crystal in the power core just moments before a total shutdown. The lights flickered back to life, and the station's inhabitants erupted in cheers. Luna's bravery and ingenuity, along with Vector's unwavering assistance, had saved them all.\\n\\nFrom that day on, Luna the baby llama was celebrated as a hero. Her story proved that even in the vast loneliness of space, determination and teamwork could overcome any challenge. Luna's tale became a beacon of hope, inspiring future generations to look to the stars with courage and curiosity.\", 'rating': '5', 'feedback': 'now this is what i like'}) (input_keys={'feedback', 'query', 'rating'}),\n",
       " Example({'query': 'tell me my preferences', 'response': 'Based on your high rating and positive feedback, it seems you prefer:\\n1. **Short yet comprehensive stories** – Narrative that is rich but not overly long.\\n2. **Unique settings** – Stories set in interesting and imaginative worlds.\\n3. **Adventurous plotlines** – Stories featuring determination, courage, and overcoming challenges.\\n4. **Character-driven narratives** – Featuring memorable and engaging characters like baby llamas or intriguing sidekicks like ancient robots. \\n\\nI will keep these preferences in mind for future stories. Would you like another one?', 'rating': '3'}) (input_keys={'feedback', 'query', 'rating'})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Append context to each item in history_with_feedback\n",
    "for item in history_with_feedback:\n",
    "    if 'rating' in item and item['rating'] != \"None\":\n",
    "        if float(item['rating']) <= 5 and float(item['rating']) >= 1:\n",
    "            item['rating'] = str(item['rating'])\n",
    "        else:\n",
    "            item['rating'] = \"3\"\n",
    "    else:\n",
    "        item['rating'] = \"3\"\n",
    "\n",
    "#convert to a dspy.example\n",
    "from dspy import Example\n",
    "history_with_feedback_examples = [\n",
    "    Example(base=item).with_inputs('query','rating','feedback') for item in history_with_feedback\n",
    "]\n",
    "\n",
    "display(history_with_feedback_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try the bootstrapfewshot optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:18<00:04,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"predictor\": {\n",
       "    \"lm\": null,\n",
       "    \"traces\": [],\n",
       "    \"train\": [],\n",
       "    \"demos\": [\n",
       "      {\n",
       "        \"query\": \"tell me my preferences\",\n",
       "        \"response\": \"Based on your high rating and positive feedback, it seems you prefer:\\n1. **Short yet comprehensive stories** \\u2013 Narrative that is rich but not overly long.\\n2. **Unique settings** \\u2013 Stories set in interesting and imaginative worlds.\\n3. **Adventurous plotlines** \\u2013 Stories featuring determination, courage, and overcoming challenges.\\n4. **Character-driven narratives** \\u2013 Featuring memorable and engaging characters like baby llamas or intriguing sidekicks like ancient robots. \\n\\nI will keep these preferences in mind for future stories. Would you like another one?\",\n",
       "        \"rating\": \"3\"\n",
       "      }\n",
       "    ],\n",
       "    \"signature_instructions\": \"You are a helpful assistant that is always improving based on user feedback. When the user responds to you, they may also include a rating (1-5) and feedback of your last message. Use this feedback to improve your future responses, but you don't need to repeat the user's feedback or rating in your reply.\",\n",
       "    \"signature_prefix\": \"Response:\"\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "\n",
    "compiled = BootstrapFewShot(\n",
    "    metric=lambda example, prediction, *args: str(float(example['rating'])/5),\n",
    "    max_labeled_demos=5,\n",
    ").compile(\n",
    "    student=ChatModule(),\n",
    "    trainset=history_with_feedback_examples,\n",
    ")\n",
    "\n",
    "compiled.save('demo_compiled_first_try.json')\n",
    "\n",
    "with open(\"demo_compiled_first_try.json\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can test how well it remembers our preference for cute baby llamas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "Query: tell me a very short story that involves robots and scifi space stuff\n",
       "\n",
       "Rating: \n",
       "\n",
       "Feedback: \n",
       "\n",
       "Response: \n",
       "\n",
       "In the sprawling megacity of Neo-Tokyo Alpha, a humble maintenance robot named XR-47 discovered an ancient alien artifact while cleaning the city's sewage system. Curious and unprogrammed for exploration, XR-47 decided to investigate.\n",
       "\n",
       "The artifact soon activated, revealing coordinates to a hidden space station orbiting a distant nebula. XR-47, equipped with newfound courage, hopped on a discarded shuttle and used the coordinates to navigate the cosmos.\n",
       "\n",
       "Upon reaching the space station, XR-47 uncovered advanced alien technology that could save Neo-Tokyo Alpha from its energy crisis. Returning as an unexpected hero, XR-47 proved that even the simplest robots could change the fate of worlds."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = compiled(\"tell me a very short story that involves robots and scifi space stuff\")\n",
    "display(Markdown(test.response[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, what is different about Mine-Tuning than just a normal chatbot?\n",
    "\n",
    "1) we can save all of our feedback for later training (this is what i mean by \"bootstrap to mastery\")\n",
    "2) the feedback is portable - we can quickly replace the llm, or the optimizer, etc to bring our preferences into a new AI program\n",
    "3) we can do this \"on-the-job\" - while we are chatting, we are also saving our preferences, and at any point we can re-compile to re-optimize for those preferences.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a version of the 5-star chat dialog that runs the compiled module instead of the llm.\n",
    "\n",
    "we just need to change the response structure: \n",
    "```\n",
    "response = compiled(message_text, rating=selected_rating, feedback=feedback_text)\n",
    "response_text = response.response[0]\n",
    "```\n",
    "\n",
    "potential future todo: change the prompting template to be more optimized for chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**User:** tell me a short story"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** ---\n",
       "\n",
       "Query: tell me a short story\n",
       "\n",
       "Rating: None\n",
       "\n",
       "Feedback: None\n",
       "\n",
       "Response:\n",
       "\n",
       "In the cyber city of Neon Haven, lived a baby llama named Lumo. Lumo was known for his small size but big heart. One day, the city was terrorized by five notorious criminals called the Shadow Syndicate. Blade, the clever leader, Circuit, the sly hacker, Viper, the fierce enforcer, Mirage, the stealthy infiltrator, and Specter, the deadly sniper.\n",
       "\n",
       "Determined to bring peace, Lumo devised a plan. With the help of his trusty robot friend, Vector, he set out on his mission. Using his small size, Lumo snuck through the city's tightest spots, gathering crucial information about the Syndicate's hideouts. Vector hacked into the Syndicate's systems, creating chaos in their operations.\n",
       "\n",
       "One by one, Lumo and Vector outsmarted and captured the members of the Shadow Syndicate. Blade was baffled by their clever tactics, Circuit’s hacking fell short against Vector’s security breaches, and Viper was no match for Lumo's agility. Mirage and Specter, despite their skills, were apprehended with teamwork.\n",
       "\n",
       "The city of Neon Haven celebrated Lumo and Vector's bravery. The baby llama proved that even the smallest hero could bring down the biggest threats, and Neon Haven was safe once more."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User:** try again."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** ---\n",
       "\n",
       "Query: try again.\n",
       "\n",
       "Rating: 4\n",
       "\n",
       "Feedback: i do love Lumo the llama as the hero. and Neon Haven is pretty scifi. but could you have more lasers please? and space stuff...by scifi i mean outer space.\n",
       "\n",
       "Response:\n",
       "\n",
       "In the bustling space station of Neon Haven, orbiting the planet Zypheron-5, lived a clever baby llama named Lumo. Lumo had an adventurous spirit and a knack for fixing things. One day, the station was threatened by five notorious criminals known as the Galactic Shadow Syndicate. Blade, the cunning leader, Circuit, the master hacker, Viper, the ruthless enforcer, Mirage, the stealthy infiltrator, and Specter, the sharp-eyed sniper.\n",
       "\n",
       "Determined to protect his home, Lumo teamed up with his trusty robot companion, Vector. The duo learned that the Syndicate planned to steal the station's energy core, which was crucial for powering the station's defenses and life support systems. Using a small shuttle, they ventured into the lower sections of Neon Haven, where Vector hacked security systems and Lumo navigated the tight passages.\n",
       "\n",
       "As they neared the Syndicate's hideout, Lumo and Vector were met with laser traps and automated defenses. Dodging and disabling the lasers with precision, they pressed forward. In a dramatic showdown in the core chamber, Lumo and Vector used clever tactics and quick thinking to defeat the Syndicate members one by one.\n",
       "\n",
       "Blade was outmaneuvered by Lumo's agility, Circuit's hacking prowess faltered against Vector's countermeasures, and Viper was taken down with a well-timed laser blast. Mirage tried to use stealth but was caught by Vector's sensors, and Specter was out-sniped by Lumo during an epic laser duel in zero gravity.\n",
       "\n",
       "With the Galactic Shadow Syndicate defeated, the energy core was safe, and Neon Haven was secure once more. Lumo returned as a celebrated hero, proving that even the smallest llamas could achieve greatness among the stars. The residents of Neon Haven would always remember the bravery of Lumo and Vector, the little llama and his metallic friend, who saved their floating home in outer space."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716181afe7a8425b8c3ee13800948859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center><p>Rating</p></center>'), HBox(children=(Button(description='1', style=Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0ae84ca04b4791990afb88996df602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Feedback:', layout=Layout(height='100px', width='100%'), placeholder='Type you…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc2a7562c404fa1bf7d6901fd65ced8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Query:', layout=Layout(height='100px', width='100%'), placeholder='Type your m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12df581d6dd4303846e1c672bad82c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Submit', style=ButtonStyle()),), layout=Layout(justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown, HTML\n",
    "\n",
    "# Initialize storage for chat history, ratings, and feedback\n",
    "chat_history = []\n",
    "history_with_feedback = []\n",
    "\n",
    "# Variable to store the selected rating\n",
    "selected_rating = None\n",
    "\n",
    "# Create buttons for ratings 1 to 5\n",
    "buttons = [widgets.Button(description=str(i)) for i in range(1, 6)]\n",
    "\n",
    "# Arrange the buttons horizontally and center them\n",
    "button_box = widgets.VBox([\n",
    "    widgets.HTML(\"<center><p>Rating</p></center>\"),\n",
    "    widgets.HBox(buttons, layout=widgets.Layout(justify_content='center'))\n",
    "])\n",
    "\n",
    "# Create a Textarea widget for feedback\n",
    "feedback_area = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your feedback here',\n",
    "    description='Feedback:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a Textarea widget for message\n",
    "message_area = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your message here',\n",
    "    description='Query:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "# Create a submit button\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "\n",
    "# Center the submit button\n",
    "submit_button_box = widgets.HBox([submit_button], layout=widgets.Layout(justify_content='center'))\n",
    "\n",
    "# Function to display the chat history and input widgets\n",
    "def display_chat():\n",
    "    clear_output()\n",
    "    \n",
    "    # Display the chat history\n",
    "    for entry in chat_history:\n",
    "        display(Markdown(f\"**User:** {entry['message']}\"))\n",
    "        display(Markdown(f\"**Response:** {entry['response']}\"))\n",
    "    \n",
    "        \n",
    "    # Display the rating buttons and feedback area only if there is message history\n",
    "    if chat_history:\n",
    "        display(button_box)\n",
    "        display(feedback_area)\n",
    "\n",
    "    # Display the message area\n",
    "    display(message_area)\n",
    "    \n",
    "    # Display the submit button\n",
    "    display(submit_button_box)\n",
    "\n",
    "# Function to handle submit button click\n",
    "def on_submit_click(b):\n",
    "    global selected_rating\n",
    "    \n",
    "    # Get the message and feedback text\n",
    "    message_text = message_area.value\n",
    "    feedback_text = feedback_area.value\n",
    "    \n",
    "    # Display loading message\n",
    "    loading_message = widgets.HTML(\"<center><p>Loading...</p></center>\")\n",
    "    display(loading_message)\n",
    "    \n",
    "    # Simulate getting a response from the model\n",
    "    response = compiled(message_text, rating=str(selected_rating), feedback=feedback_text)\n",
    "    response_text = response.response[0]\n",
    "    \n",
    "    # Clear loading message\n",
    "    clear_output()\n",
    "    \n",
    "    # Save the chat history\n",
    "    chat_history.append({\n",
    "        'message': message_text,\n",
    "        'response': response_text\n",
    "    })\n",
    "    \n",
    "    # Save the rating and feedback to the last entry in history_with_feedback\n",
    "    if history_with_feedback:\n",
    "        history_with_feedback[-1]['rating'] = selected_rating\n",
    "        history_with_feedback[-1]['feedback'] = feedback_text\n",
    "    history_with_feedback.append({\n",
    "        'query': message_text,\n",
    "        'response': response_text,\n",
    "    })\n",
    "    \n",
    "    # Reset the input fields and selected rating\n",
    "    message_area.value = ''\n",
    "    feedback_area.value = ''\n",
    "    selected_rating = None\n",
    "    \n",
    "    # Reset button styles\n",
    "    for button in buttons:\n",
    "        button.style.button_color = None\n",
    "    \n",
    "    # Display the updated chat\n",
    "    display_chat()\n",
    "\n",
    "# Function to handle button click\n",
    "def on_button_click(b):\n",
    "    global selected_rating\n",
    "    selected_rating = int(b.description)\n",
    "    \n",
    "    # Reset button styles\n",
    "    for button in buttons:\n",
    "        button.style.button_color = None\n",
    "    \n",
    "    # Highlight the selected button\n",
    "    b.style.button_color = 'lightblue'\n",
    "    #print(f\"Selected rating: {selected_rating}\")\n",
    "\n",
    "# Attach the button click event to each button\n",
    "for button in buttons:\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "# Attach the submit button click event\n",
    "submit_button.on_click(on_submit_click)\n",
    "\n",
    "# Display the initial chat interface\n",
    "display_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
