{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need to install these\n",
    "%pip install dspy\n",
    "%pip install ipywidgets\n",
    "%pip install IPython\n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import dspy\n",
    "import os\n",
    "import dotenv\n",
    "import wget\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the context, in this case the minetuning readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/rawwerks/MineTuning/main/README.md\"\n",
    "response = requests.get(url)\n",
    "context = response.text\n",
    "with open(context_file, 'r') as file:\n",
    "    context = file.read()\n",
    "    \n",
    "from IPython.display import Markdown\n",
    "display(Markdown(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use multiple generations (`count`) to speed up the process. \n",
    "todo: first get it working with count = 1, then figure out how to reconcile multiple generations with multiple attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(\n",
       "     rationale='Query: i\\'m confused. eli5\\n\\nContext:\\n# Mine-Tuning\\n\\nMine-tuning is a methodology for synchronizing human and AI attention. Mine-tuning is a state-of-mind(s), an approach to continuous learning and improvement through on-the-job feedback. Mine-tuning offers a rapid approach to customizing AI programs to our personal preferences, professions, and priorities.\\n\\n**TL;DR - Mine-tuning makes it easy for humans to teach AI how to do a good job.**\\n\\n## Key Principles of Mine-tuning: \\n\\n1. **On-the-job learning.** In contrast to traditional fine-tuning methods which separate \"learning\" from \"doing\", the AI program is continuously trained on the user\\'s feedback -- while assisting the user in the actual task at hand.\\n2. **Immediate human-in-the-loop feedback.** Attention is all it takes. The AI program is given immediate human feedback, preferably on every response.\\n3. **Bootstrap to mastery.** Training data sets can be built up one example at a time. At the end of each interaction, the program can be re-optimized to bootstrap on its new knowledge. Past training histories can be rapidly imported to get a new AI program \"up to speed\".\\n4. **Personal, portable, and parametric.** We all of have different ways of working. Mine-tuning enables individuals to develop personalized and private AI programs that are modular in nature. These programs are parametric and portable, which means that they are not locked into any single LLM provider or model. \\n5. **Embrace cognitive diversity.** Like humans, LLMs are both opaque and non-deterministic. LLMs can rapidly learn from vast amounts of data, while humans tend to learn best iteratively, by example. Mine-tuning embraces human and AI cognitive diversity, with the goal of synchronizing attention to improve collaboration. \\n\\n## Attention is all it takes.\\n\\nAttention is all it takes. We are so hungry to leverage generative AI to create more stuff from less attention, that we tend to overlook the opportunity to give AI our full attention. When we do provide feedback to the AI, to course-correct towards a more ideal behavior, that valuable learning interaction is typically lost. This is made more challenging by the stateless, non-deterministic, and opaque nature of LLMs. \\n\\nWhen we do manage to get something we like, our options to save that are primitive: copy the share link to this chat history so we can start from where we left off? Similarly, when we finally succeed at \"prompt engineering\" one model, we find that the tricks are not transferrable to another model. \\n\\nMine-tuning is more work up front, because it requires more attention per interaction. In the same way that it is more work to onboard a new employee, the user provides immediate feedback to the AI program during mine-tuning. \\n\\nThe canonical mine-tuning example is an interaction where each AI response is given both a quantitative rating (ie 1-5) and qualitative feedback. Optionally, the program retries until a perfect rating (ie 5 stars) is achieved. The program can then be re-optimized using those iterative training examples, either with the response rating as the optimization metric, or another score metric (for example, the reciprocal of the number of attempts required to achieve a perfect rating).\\n\\n## Connections to and distinctions from existing methods\\n- **How is mine-tuning different from fine-tuning?** The key difference is the approach to training the AI program. If you were going to train another person to do a job, would you hide in a separate room, write down 1000 examples of \"if this, then that\", then hand it to them and tell them to review it alone? No. You would show them how to do the job, have them shadow you while you do the job, then shadow them while they do the job. With mine-tuning, training datasets are built on-the-job, example by example. Once the training dataset is in place, then traditional fine-tuning can be employed if desired.\\n- **Isn\\'t this just RLHF?** Yes, thematically it is. The difference is: who is implementing the feedback system? The AI provider or the user? When you give ChatGPT a thumbs-up / thumbs-down, what happens? I certainly have no idea. Nor is it the job of the foundation model providers to tailor LLMs to the specific needs of an individual. (And as soon as one of them tries to do that, we will find it very very creepy. 1000x creepier than Alexa listening to your family\\'s conversations to sell you soap.) Your preferences are highly personal and private. You should own your feedback. You should control how that feedback is implemented by the AI program, and where the resulting datasets and mine-tuned AI programs are stored.\\n- **Isn\\'t it just few-shot prompting?** Few-shot prompting is foundational. One key distinction is that every shot gets a score, which enables AI programs to be re-optimized iteratively, in between each user interaction. \\n- **What is the connection to DSPy?** [DSPy](https://dspy-docs.vercel.app) is a very convenient and powerful way to implement many of the ideas of mine-tuning. Mine-tuning is easily enabled by DSPy\\'s ability to write AI programs instead of prompts, to optimize those programs towards a specific metric, and to swap out program parameters (ie, switch to a different LLM, or different optimization algorithm).\\n- **What is the connection to Wekinator?** [Wekinator](http://www.wekinator.org/) was a revolutionary approach to real-time interactive machine learning, developed at Princeton in 2008 by [Rebecca Fiebrink](https://researchers.arts.ac.uk/1594-rebecca-fiebrink/). The \"play along learning\" paradigm pioneered by Wekinator inspired the on-the-job learning approach of mine-tuning.\\n- **How is this different from a personalized AI framework?** It doesn\\'t have to be different. You could argue that [MemGPT](https://memgpt.ai/) is mine-tuned to your preferences. At the same time, mine-tuning doesn\\'t need to involve agentic frameworks, tools, or long-term memory. Mine-tuning could simply be a program that transforms data just the way you like it.\\n\\n## Examples\\n- _[\"Sense of Humor\"](https://github.com/rawwerks/MineTuning/tree/main/examples/sense_of_humor)_ - A playful mine-tuning demo where you can quickly orient an AI to your personal sense of humor. \\n- Many more coming soon...\\n\\n---\\n\\n### Inspiration\\n- [DSPy](https://dspy-docs.vercel.app)\\n- [MemGPT](https://memgpt.ai/)\\n- [Wekinator](http://www.wekinator.org/)\\n\\n### About\\nMine-tuning is a vision for cognitive collaboration, first translated into English and Python by [Raymond Weitekamp](https://raw.works/).\\n\\nRationale:\\nThe user is asking for an \"explain like I\\'m 5\" (ELI5) explanation of the concept of mine-tuning. Given the detailed and technical nature of the provided context, the response should simplify the explanation using basic terms and relatable analogies to ensure clarity.',\n",
       "     response=\"Mine-tuning is like teaching a new helper (the AI) how to do things exactly the way you like them, while you're doing those things. Imagine you have a friend helping you bake cookies. Instead of giving them a huge recipe book and leaving them alone, you show them step-by-step how you like your cookies to be made. You taste each cookie they make, give them a thumbs up or down, and tell them what to change. Over time, your friend learns to make the cookies just the way you like them. Mine-tuning is similar, but for AI: you give it feedback on each response, and it learns to do things more and more the way you prefer.\"\n",
       " ),\n",
       " Prediction(\n",
       "     rationale='Query: i\\'m confused. eli5\\n\\nContext:\\n# Mine-Tuning\\n\\nMine-tuning is a methodology for synchronizing human and AI attention. Mine-tuning is a state-of-mind(s), an approach to continuous learning and improvement through on-the-job feedback. Mine-tuning offers a rapid approach to customizing AI programs to our personal preferences, professions, and priorities.\\n\\n**TL;DR - Mine-tuning makes it easy for humans to teach AI how to do a good job.**\\n\\n## Key Principles of Mine-tuning: \\n\\n1. **On-the-job learning.** In contrast to traditional fine-tuning methods which separate \"learning\" from \"doing\", the AI program is continuously trained on the user\\'s feedback -- while assisting the user in the actual task at hand.\\n2. **Immediate human-in-the-loop feedback.** Attention is all it takes. The AI program is given immediate human feedback, preferably on every response.\\n3. **Bootstrap to mastery.** Training data sets can be built up one example at a time. At the end of each interaction, the program can be re-optimized to bootstrap on its new knowledge. Past training histories can be rapidly imported to get a new AI program \"up to speed\".\\n4. **Personal, portable, and parametric.** We all of have different ways of working. Mine-tuning enables individuals to develop personalized and private AI programs that are modular in nature. These programs are parametric and portable, which means that they are not locked into any single LLM provider or model. \\n5. **Embrace cognitive diversity.** Like humans, LLMs are both opaque and non-deterministic. LLMs can rapidly learn from vast amounts of data, while humans tend to learn best iteratively, by example. Mine-tuning embraces human and AI cognitive diversity, with the goal of synchronizing attention to improve collaboration. \\n\\n## Attention is all it takes.\\n\\nAttention is all it takes. We are so hungry to leverage generative AI to create more stuff from less attention, that we tend to overlook the opportunity to give AI our full attention. When we do provide feedback to the AI, to course-correct towards a more ideal behavior, that valuable learning interaction is typically lost. This is made more challenging by the stateless, non-deterministic, and opaque nature of LLMs. \\n\\nWhen we do manage to get something we like, our options to save that are primitive: copy the share link to this chat history so we can start from where we left off? Similarly, when we finally succeed at \"prompt engineering\" one model, we find that the tricks are not transferrable to another model. \\n\\nMine-tuning is more work up front, because it requires more attention per interaction. In the same way that it is more work to onboard a new employee, the user provides immediate feedback to the AI program during mine-tuning. \\n\\nThe canonical mine-tuning example is an interaction where each AI response is given both a quantitative rating (ie 1-5) and qualitative feedback. Optionally, the program retries until a perfect rating (ie 5 stars) is achieved. The program can then be re-optimized using those iterative training examples, either with the response rating as the optimization metric, or another score metric (for example, the reciprocal of the number of attempts required to achieve a perfect rating).\\n\\n## Connections to and distinctions from existing methods\\n- **How is mine-tuning different from fine-tuning?** The key difference is the approach to training the AI program. If you were going to train another person to do a job, would you hide in a separate room, write down 1000 examples of \"if this, then that\", then hand it to them and tell them to review it alone? No. You would show them how to do the job, have them shadow you while you do the job, then shadow them while they do the job. With mine-tuning, training datasets are built on-the-job, example by example. Once the training dataset is in place, then traditional fine-tuning can be employed if desired.\\n- **Isn\\'t this just RLHF?** Yes, thematically it is. The difference is: who is implementing the feedback system? The AI provider or the user? When you give ChatGPT a thumbs-up / thumbs-down, what happens? I certainly have no idea. Nor is it the job of the foundation model providers to tailor LLMs to the specific needs of an individual. (And as soon as one of them tries to do that, we will find it very very creepy. 1000x creepier than Alexa listening to your family\\'s conversations to sell you soap.) Your preferences are highly personal and private. You should own your feedback. You should control how that feedback is implemented by the AI program, and where the resulting datasets and mine-tuned AI programs are stored.\\n- **Isn\\'t it just few-shot prompting?** Few-shot prompting is foundational. One key distinction is that every shot gets a score, which enables AI programs to be re-optimized iteratively, in between each user interaction. \\n- **What is the connection to DSPy?** [DSPy](https://dspy-docs.vercel.app) is a very convenient and powerful way to implement many of the ideas of mine-tuning. Mine-tuning is easily enabled by DSPy\\'s ability to write AI programs instead of prompts, to optimize those programs towards a specific metric, and to swap out program parameters (ie, switch to a different LLM, or different optimization algorithm).\\n- **What is the connection to Wekinator?** [Wekinator](http://www.wekinator.org/) was a revolutionary approach to real-time interactive machine learning, developed at Princeton in 2008 by [Rebecca Fiebrink](https://researchers.arts.ac.uk/1594-rebecca-fiebrink/). The \"play along learning\" paradigm pioneered by Wekinator inspired the on-the-job learning approach of mine-tuning.\\n- **How is this different from a personalized AI framework?** It doesn\\'t have to be different. You could argue that [MemGPT](https://memgpt.ai/) is mine-tuned to your preferences. At the same time, mine-tuning doesn\\'t need to involve agentic frameworks, tools, or long-term memory. Mine-tuning could simply be a program that transforms data just the way you like it.\\n\\n## Examples\\n- _[\"Sense of Humor\"](https://github.com/rawwerks/MineTuning/tree/main/examples/sense_of_humor)_ - A playful mine-tuning demo where you can quickly orient an AI to your personal sense of humor. \\n- Many more coming soon...\\n\\n---\\n\\n### Inspiration\\n- [DSPy](https://dspy-docs.vercel.app)\\n- [MemGPT](https://memgpt.ai/)\\n- [Wekinator](http://www.wekinator.org/)\\n\\n### About\\nMine-tuning is a vision for cognitive collaboration, first translated into English and Python by [Raymond Weitekamp](https://raw.works/).\\n\\nRationale: The user is confused and has asked for an ELI5 (Explain Like I\\'m 5) explanation. The context provided is detailed and technical. The response should simplify the concepts and principles of mine-tuning, using easy-to-understand language and analogies suitable for a young audience.',\n",
       "     response=\"Imagine you have a robot friend who wants to help you with your homework. At first, the robot doesn't know exactly how you like things done, so you need to teach it. Instead of giving it a huge book of instructions, you show it how to do one thing at a time while you work together.\\n\\n1. **Learning While Doing:** As you do your homework, you tell the robot if it's doing a good job or not right away. This way, the robot learns and improves while helping you.\\n2. **Immediate Feedback:** Every time the robot helps you, you give it a thumbs up or down and tell it why. This helps the robot understand what you like and don't like.\\n3. **Step-by-Step Improvement:** Each time the robot helps, it gets better. It remembers what you taught it and keeps getting smarter with each task.\\n4. **Personalized for You:** The robot learns your unique way of doing things, so it can help you in a way that fits you best. Plus, you can take this robot friend anywhere, and it will still know how you like things done.\\n5. **Working Together:** You and the robot think differently, but that's okay! By working together and paying attention to each other, you can do great things.\\n\\nIn short, mine-tuning is like teaching a robot friend to help you perfectly, by giving it lots of attention and feedback while you work together.\"\n",
       " )]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#setup dspy\n",
    "dotenv.load_dotenv(os.path.expanduser(\"~/.env\"))  # load OpenAI API key from .env file, or you can set directly with \n",
    "llm = dspy.OpenAI(model='gpt-4o', max_tokens=4096, temperature=0.5)\n",
    "#llm = dspy.OpenAI(model='gpt-4o', max_tokens=4096, api_key=\"sk-\") #or you can set the API key directly here\n",
    "dspy.settings.configure(lm=llm)\n",
    "\n",
    "count = 2\n",
    "\n",
    "query = input(\"Query:\") #from interactive input\n",
    "#query = \"steampunk chic superheroes.\" #alternatively write the string directly into the code\n",
    "\n",
    "#signature\n",
    "class Visualization(dspy.Signature):\n",
    "    \"\"\"You are an expert who is always learning and improving based on user feedback.\"\"\"\n",
    "    query = dspy.InputField(description=\"this is the user's query.\")\n",
    "    context = dspy.InputField(description=\"this is the context of the query. study it carefully.\")\n",
    "    rationale = dspy.OutputField(description=\"this is your rationale for your response.\")\n",
    "    response = dspy.OutputField(description=\"this is your response.\")\n",
    "    history = dspy.InputField(required=False, description=\"this is the history of the conversation with the user. use it to improve your next response.\")\n",
    "\n",
    "#module \n",
    "class VisualizerModule(dspy.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.signature = Visualization\n",
    "        self.predictor = dspy.ChainOfThought(self.signature, n=count)\n",
    "\n",
    "    def forward(self, query, context, score=None, feedback=None):\n",
    "        result = self.predictor(query=query, context=context, score=score, feedback=feedback, num_generations=count)\n",
    "        #print(result.completions)\n",
    "        return [dspy.Prediction(rationale=completion.rationale, response=completion.response) for completion in result.completions]\n",
    "\n",
    "#instance\n",
    "visualizer = VisualizerModule()\n",
    "\n",
    "response = visualizer(query=query, context=context)\n",
    "display(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, we are going to calculate the `score` to be the reciprocal of the number of attempts to achieve a 5 star review. for all responses with less than 5 stars, the score is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_rating(response):\n",
    "    while True:\n",
    "        try:\n",
    "            rating = int(input(f\"{response}\\n---\\nPlease rate on a 5-point scale (1-5): \"))\n",
    "            if 1 <= rating <= 5:\n",
    "                return rating  # Cast the integer to a string before returning\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a number between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "def get_user_feedback(response):\n",
    "    feedback = input(f\"{response}\\n---\\nPlease provide your feedback: \")\n",
    "    return feedback\n",
    "\n",
    "def get_score(attempts, rating):\n",
    "    if rating >= 5:\n",
    "        score = 1/attempts\n",
    "    else:\n",
    "        score = 0\n",
    "    print(\"calculated score: \" + str(score) + \" based on rating: \" + str(rating) + \" and attempts: \" + str(attempts))\n",
    "    return score\n",
    "\n",
    "history = []\n",
    "\n",
    "# Get rating and feedback for each response\n",
    "for response in response.completions:\n",
    "    while True:\n",
    "        print(response)\n",
    "        rating = get_user_rating(response)\n",
    "        feedback = get_user_feedback(response)\n",
    "        this_example = {\n",
    "            'query': query,\n",
    "            #'context': context, #this is omitted for clarity, but might be useful to include if the context were not static. (ie, RAG)\n",
    "            'rationale': response.rationale,\n",
    "            'response': response,\n",
    "            'rating': rating,\n",
    "            'feedback': feedback\n",
    "        }\n",
    "        history.append(this_example)\n",
    "        if rating >= 5:\n",
    "            print(\"history:\")\n",
    "            print(history)\n",
    "            break\n",
    "        history_str = ' '.join(str(item) for item in history)  # Convert each item to string before joining\n",
    "        attempt_count += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
