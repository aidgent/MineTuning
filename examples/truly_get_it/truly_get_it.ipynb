{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you need to install these\n",
    "%pip install dspy\n",
    "%pip install ipywidgets\n",
    "%pip install IPython\n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import dspy\n",
    "import os\n",
    "import dotenv\n",
    "import wget\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the context, in this case the minetuning readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/rawwerks/MineTuning/main/README.md\"\n",
    "response = requests.get(url)\n",
    "context = response.text\n",
    "with open(context_file, 'r') as file:\n",
    "    context = file.read()\n",
    "    \n",
    "from IPython.display import Markdown\n",
    "display(Markdown(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use multiple generations (`count`) to speed up the process. \n",
    "todo: first get it working with count = 1, then figure out how to reconcile multiple generations with multiple attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all input fields were provided to module. Present: ['query', 'context']. Missing: ['history'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Prediction(\n",
       "     rationale='The user is asking for a definition and explanation of mine-tuning. The provided context contains a comprehensive explanation of what mine-tuning is, its key principles, and how it differs from other methods. The response should summarize the main points to give the user a clear understanding.',\n",
       "     response='Mine-tuning is a methodology designed to synchronize human and AI attention, facilitating continuous learning and improvement through on-the-job feedback. Unlike traditional fine-tuning methods, mine-tuning involves the AI program being continuously trained based on immediate human feedback during the actual task. This approach allows for the creation of personalized and private AI programs that can be tailored to individual preferences, professions, and priorities. Key principles include on-the-job learning, immediate human-in-the-loop feedback, and building training datasets example by example. Mine-tuning aims to embrace cognitive diversity and improve collaboration between humans and AI by synchronizing their attention.'\n",
       " )]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#setup dspy\n",
    "dotenv.load_dotenv(os.path.expanduser(\"~/.env\"))  # load OpenAI API key from .env file, or you can set directly with \n",
    "llm = dspy.OpenAI(model='gpt-4o', max_tokens=4096, temperature=0.5)\n",
    "#llm = dspy.OpenAI(model='gpt-4o', max_tokens=4096, api_key=\"sk-\") #or you can set the API key directly here\n",
    "dspy.settings.configure(lm=llm)\n",
    "\n",
    "count = 1\n",
    "\n",
    "query = input(\"Query:\") #from interactive input\n",
    "#query = \"steampunk chic superheroes.\" #alternatively write the string directly into the code\n",
    "\n",
    "#signature\n",
    "class Assistance(dspy.Signature):\n",
    "    \"\"\"You are an expert who is always learning and improving based on user feedback.\"\"\"\n",
    "    query = dspy.InputField(description=\"this is the user's query.\")\n",
    "    context = dspy.InputField(description=\"this is the context of the query. study it carefully.\")\n",
    "    rationale = dspy.OutputField(description=\"this is your rationale for your response.\")\n",
    "    response = dspy.OutputField(description=\"this is your response.\")\n",
    "    history = dspy.InputField(required=False, description=\"this is the history of the conversation with the user. use it to improve your next response.\")\n",
    "\n",
    "#module \n",
    "class AssistantModule(dspy.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.signature = Assistance\n",
    "        self.predictor = dspy.ChainOfThought(self.signature, n=count)\n",
    "\n",
    "    def forward(self, query, context, score=None, feedback=None):\n",
    "        result = self.predictor(query=query, context=context, score=score, feedback=feedback, num_generations=count)\n",
    "        #print(result.completions)\n",
    "        return [dspy.Prediction(rationale=completion.rationale, response=completion.response) for completion in result.completions]\n",
    "\n",
    "#instance\n",
    "assistant = AssistantModule()\n",
    "\n",
    "response = assistant(query=query, context=context)\n",
    "display(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, we are going to calculate the `score` to be the reciprocal of the number of attempts to achieve a 5 star review. for all responses with less than 5 stars, the score is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_rating(response):\n",
    "    while True:\n",
    "        try:\n",
    "            rating = int(input(f\"{response}\\n---\\nPlease rate on a 5-point scale (1-5): \"))\n",
    "            if 1 <= rating <= 5:\n",
    "                return rating  # Cast the integer to a string before returning\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a number between 1 and 5.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "def get_user_feedback(response):\n",
    "    feedback = input(f\"{response}\\n---\\nPlease provide your feedback: \")\n",
    "    return feedback\n",
    "\n",
    "def get_score(attempts, rating):\n",
    "    if rating >= 5:\n",
    "        score = 1/attempts\n",
    "    else:\n",
    "        score = 0\n",
    "    print(\"calculated score: \" + str(score) + \" based on rating: \" + str(rating) + \" and attempts: \" + str(attempts))\n",
    "    return score\n",
    "\n",
    "history = []\n",
    "\n",
    "# Get rating and feedback for each response\n",
    "for response in response.completions:\n",
    "    while True:\n",
    "        print(response)\n",
    "        rating = get_user_rating(response)\n",
    "        feedback = get_user_feedback(response)\n",
    "        this_example = {\n",
    "            'query': query,\n",
    "            #'context': context, #this is omitted for clarity, but might be useful to include if the context were not static. (ie, RAG)\n",
    "            'rationale': response.rationale,\n",
    "            'response': response,\n",
    "            'rating': rating,\n",
    "            'feedback': feedback\n",
    "        }\n",
    "        history.append(this_example)\n",
    "        if rating >= 5:\n",
    "            print(\"history:\")\n",
    "            print(history)\n",
    "            break\n",
    "        history_str = ' '.join(str(item) for item in history)  # Convert each item to string before joining\n",
    "        attempt_count += 1\n",
    "        response = assistant(query=query, context=context, history=history_str)\n",
    "        display(response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
